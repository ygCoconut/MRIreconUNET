{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is now replaced by the script named runall.py. However, this notebook will serve as reference code to implement the inference tests. The inference tests will be performed on the model checkpoints to analyse network performance, network training progress and image quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHTUNG: http://thomas-cokelaer.info/blog/2011/09/382/\n",
    "# Errors when reloading module with the class in jupyter notebook !\n",
    "from models import modelRepository as mr\n",
    "from myCode import myFunctions\n",
    "from myCode import myDataLoader\n",
    "from models import UNet3d_parts\n",
    "from models import UNet3d_assembled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import scipy\n",
    "import scipy.io as spio\n",
    "\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorboard\n",
    "### Tensorboard from TF 1.0 has been used, as TF 2.0 is not fully compatible with conda yet. This will avoid possible issues when copying the conda environment over to the computer cluster O2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import summary\n",
    "from tensorboard import notebook\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorboardX import FileWriter\n",
    "\n",
    "%load_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import skorch\n",
    "from skorch import NeuralNet\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skorch.callbacks import EpochScoring\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skorch.callbacks\n",
    "\n",
    "import torchbearer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dataset Class according to the following instructions from the pytorch \"DATA LOADING AND PROCESSING TUTORIAL\" notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMRIreconDataset(Dataset):\n",
    "    \"\"\"CMRIrecon dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, input_file_path, target_file_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.inputs = np.load(input_file_path)\n",
    "        self.targets = np.load(target_file_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "#         print(\"print length of inputs\",len(self.inputs))\n",
    "#         print(\"print shape of inputs\",np.shape(self.inputs))\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "#         sample = {'input': self.inputs[idx], 'target': self.targets[idx]}\n",
    "        X = self.inputs[idx]\n",
    "        Y = self.targets[idx]\n",
    "        return  X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 20, 96, 96)\n",
      "(671, 20, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "CMRIdataset = CMRIreconDataset(\n",
    "        input_file_path = \\\n",
    "        'C:/Users/littl/Documents/PythonScripts/reconproject_data/input_data.npy', \\\n",
    "        target_file_path = \\\n",
    "        'C:/Users/littl/Documents/PythonScripts/reconproject_data/target_data.npy')\n",
    "\n",
    "# print(CMRIdataset[:]['input'].shape)\n",
    "X, Y = CMRIdataset[:][:]\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into train set ( 80% ) and validation set ( 20% )\n",
    "\n",
    "## This Method is possibly not ideal yet, as the two generated dataset do not have the same amount of of slices from each heart layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(CMRIdataset))\n",
    "val_size = len(CMRIdataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(CMRIdataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "# define loaders\n",
    "# ! numworkers set to 0 for windows !!\n",
    "# load training set\n",
    "trainloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "# load validation set\n",
    "valloader = DataLoader(val_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "# print(train_dataset[:]['input'].shape)\n",
    "# print(val_dataset[:]['input'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note about NN input.\n",
    "## c.f. NN tutorial from pytorch\n",
    "\n",
    "``torch.nn`` only supports mini-batches. The entire ``torch.nn`` package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "a fake batch dimension.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model from modelRepository.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = mr.BNv0()\n",
    "net2 = mr.BNv1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'models.modelRepository.BN20channels'>\n"
     ]
    }
   ],
   "source": [
    "channels = 20\n",
    "model = UNet3d_assembled.UNet3d(channels)\n",
    "# override\n",
    "model = net2\n",
    "\n",
    "\n",
    "model = mr.BN20channels()\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.rand(1).cuda()\n",
    "# # DOES NOT WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This project is not yet compatible with train and val data at the same time \n",
    "# and can only overfit one dataset at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# INCLUDE code for multiple GPUs !! #\n",
    "\n",
    "# move computation to gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'models.modelRepository.BN20channels'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5, 20, 96, 96)\n",
      "(5, 20, 96, 96)\n",
      "(4, 20, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# X, Y = CMRIdataset\n",
    "X, Y = CMRIdataset[:5][:5]\n",
    "# X = torch.from_numpy(X.astype(np.float32))\n",
    "# Y = torch.from_numpy(Y.astype(np.float32))\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.float32)\n",
    "\n",
    "\n",
    "print(type(model))\n",
    "print(type(X))\n",
    "print(type(Y))\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X.numpy(), Y.numpy(), test_size = 0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# X = torch.from_numpy(X).float.to(device)\n",
    "# Y = torch.from_numpy(Y).float.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20, 96, 96)\n",
      "(5, 20, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "# skorch_callbacks\n",
    "# mean absolute error between train image and val image\n",
    "\n",
    "# def mean_abs_error_train(x_train, y_train):\n",
    "# #     NeuralNet.initialize_module(model)\n",
    "#     y_pred = model.predict(x_train)\n",
    "# #     Y_pred = model(x_train)\n",
    "#     return mean_absolute_error(y_train, x_train)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "# print(model.shape)\n",
    "\n",
    "def mean_abs_error_train(X, Y):\n",
    "    Y_pred = model.predict(X)\n",
    "    print(Y_pred.shape)\n",
    "    return mean_absolute_error(Y, Y_pred)\n",
    "#                         multioutput = 'raw_values'\n",
    "                        \n",
    "\n",
    "mean_abs_error_train_scorer = make_scorer(mean_abs_error_train)\n",
    "\n",
    "epoch_MAE_train = EpochScoring(\n",
    "        mean_abs_error_train_scorer,\n",
    "        name = 'MAE_train',\n",
    "        lower_is_better = True,\n",
    "        on_train = True,\n",
    "        use_caching = True\n",
    "        )\n",
    "\n",
    "\n",
    "# cbs = [('my score', epoch_MAE_train)]\n",
    "cyclicLR = skorch.callbacks.LRScheduler(\n",
    "            policy = 'CyclicLR',\n",
    "            )\n",
    "# lr = skorch.callbacks.CyclicLR(optimizer = Adam)\n",
    "# progressbar = skorch.callbacks.ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'models.modelRepository.BN20channels'>\n"
     ]
    }
   ],
   "source": [
    "model = mr.BN20channels()\n",
    "print(type(model))\n",
    "\n",
    "model = NeuralNet(module = model,\n",
    "                criterion = nn.MSELoss,\n",
    "                max_epochs = 5,\n",
    "                lr = 0.003,\n",
    "                device = 'cuda',\n",
    "#                 callbacks = [('lr', lr)]\n",
    "#                 callbacks = [('epochfoo', epoch_MAE_train)],\n",
    "                callbacks = [\n",
    "#                     ('progressbarfoo', progressbar), \n",
    "#                     ('epochfoo', epoch_MAE_train),\n",
    "                        (cyclicLR),                    \n",
    "                    ],\n",
    "\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=BN20channels(\n",
       "    (conv1): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN32): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (BN64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (deconv2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN32_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4): Conv2d(32, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (BN20): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        1.0736        1.0159  1.7596\n",
      "      2        1.0728        1.0159  0.0409\n",
      "      3        1.0721        1.0159  0.0399\n",
      "      4        1.0714        1.0160  0.0423\n",
      "      5        1.0706        1.0161  0.0449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=BN20channels(\n",
       "    (conv1): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN32): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (BN64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (deconv2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (BN32_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4): Conv2d(32, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (BN20): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LRScheduler' object has no attribute 'previous_epoch_train_loss_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-11ae3b9c1a93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# cyclicLR.get_scheduler(model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcyclicLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprevious_epoch_train_loss_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LRScheduler' object has no attribute 'previous_epoch_train_loss_score'"
     ]
    }
   ],
   "source": [
    "# cyclicLR.get_scheduler(model)\n",
    "cyclicLR.previous_epoch_train_loss_score(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        1.0699        1.0162  0.0419\n",
      "      7        1.0692        1.0163  0.0409\n",
      "      8        1.0685        1.0165  0.0429\n",
      "      9        1.0678        1.0169  0.0459\n",
      "     10        1.0671        1.0172  0.0429\n",
      "     11        1.0664        1.0176  0.0429\n",
      "     12        1.0657        1.0180  0.0379\n",
      "     13        1.0650        1.0184  0.0519\n",
      "     14        1.0643        1.0189  0.0379\n",
      "     15        1.0637        1.0194  0.0369\n",
      "     16        1.0630        1.0199  0.0718\n",
      "     17        1.0624        1.0204  0.0369\n",
      "     18        1.0617        1.0210  0.0364\n",
      "     19        1.0611        1.0216  0.0359\n",
      "     20        1.0605        1.0222  0.0359\n",
      "     21        1.0598        1.0228  0.0359\n",
      "     22        1.0592        1.0234  0.0359\n",
      "     23        1.0586        1.0241  0.0359\n",
      "     24        1.0580        1.0247  0.0353\n",
      "     25        1.0575        1.0254  0.0349\n",
      "     26        1.0569        1.0260  0.0359\n",
      "     27        1.0563        1.0267  0.0349\n",
      "     28        1.0558        1.0273  0.0349\n",
      "     29        1.0552        1.0279  0.0359\n",
      "     30        1.0547        1.0285  0.0369\n",
      "     31        1.0542        1.0290  0.0359\n",
      "     32        1.0536        1.0295  0.0369\n",
      "     33        1.0531        1.0300  0.0359\n",
      "     34        1.0526        1.0304  0.0349\n",
      "     35        1.0521        1.0308  0.0359\n",
      "     36        1.0516        1.0311  0.0340\n",
      "     37        1.0512        1.0313  0.0349\n",
      "     38        1.0507        1.0315  0.0359\n",
      "     39        1.0502        1.0316  0.0359\n",
      "     40        1.0497        1.0317  0.0349\n",
      "     41        1.0493        1.0317  0.0359\n",
      "     42        1.0488        1.0316  0.0360\n",
      "     43        1.0484        1.0315  0.0379\n",
      "     44        1.0479        1.0313  0.0369\n",
      "     45        1.0475        1.0310  0.0349\n",
      "     46        1.0471        1.0308  0.0369\n",
      "     47        1.0466        1.0304  0.0349\n",
      "     48        1.0462        1.0301  0.0349\n",
      "     49        1.0458        1.0297  0.0349\n",
      "     50        1.0454        1.0293  0.0359\n",
      "     51        1.0450        1.0289  0.0349\n",
      "     52        1.0446        1.0285  0.0359\n",
      "     53        1.0442        1.0281  0.0359\n",
      "     54        1.0438        1.0277  0.0350\n",
      "     55        1.0434        1.0274  0.0339\n",
      "     56        1.0430        1.0271  0.0369\n",
      "     57        1.0426        1.0268  0.0369\n",
      "     58        1.0422        1.0266  0.0359\n",
      "     59        1.0418        1.0265  0.0350\n",
      "     60        1.0415        1.0264  0.0349\n",
      "     61        1.0411        1.0264  0.0359\n",
      "     62        1.0407        1.0264  0.0369\n",
      "     63        1.0403        1.0265  0.0360\n",
      "     64        1.0400        1.0266  0.0349\n",
      "     65        1.0396        1.0268  0.0349\n",
      "     66        1.0392        1.0270  0.0360\n",
      "     67        1.0389        1.0273  0.0369\n",
      "     68        1.0385        1.0276  0.0359\n",
      "     69        1.0381        1.0279  0.0349\n",
      "     70        1.0378        1.0282  0.0359\n",
      "     71        1.0374        1.0285  0.0369\n",
      "     72        1.0371        1.0288  0.0369\n",
      "     73        1.0367        1.0291  0.0409\n",
      "     74        1.0363        1.0294  0.0419\n",
      "     75        1.0360        1.0297  0.0419\n",
      "     76        1.0356        1.0300  0.0389\n",
      "     77        1.0353        1.0302  0.0389\n",
      "     78        1.0349        1.0305  0.0370\n",
      "     79        1.0346        1.0306  0.0389\n",
      "     80        1.0342        1.0308  0.0359\n",
      "     81        1.0339        1.0309  0.0399\n",
      "     82        1.0335        1.0311  0.0389\n",
      "     83        1.0332        1.0311  0.0350\n",
      "     84        1.0329        1.0312  0.0359\n",
      "     85        1.0325        1.0312  0.0358\n",
      "     86        1.0322        1.0312  0.0379\n",
      "     87        1.0318        1.0312  0.0349\n",
      "     88        1.0315        1.0312  0.0415\n",
      "     89        1.0311        1.0311  0.0379\n",
      "     90        1.0308        1.0310  0.0369\n",
      "     91        1.0305        1.0309  0.0419\n",
      "     92        1.0301        1.0308  0.0389\n",
      "     93        1.0298        1.0307  0.0349\n",
      "     94        1.0295        1.0305  0.0359\n",
      "     95        1.0291        1.0304  0.0389\n",
      "     96        1.0288        1.0302  0.0349\n",
      "     97        1.0285        1.0300  0.0339\n",
      "     98        1.0281        1.0298  0.0359\n",
      "     99        1.0278        1.0296  0.0399\n",
      "    100        1.0275        1.0294  0.0349\n",
      "    101        1.0272        1.0292  0.0349\n",
      "    102        1.0268        1.0289  0.0369\n",
      "    103        1.0265        1.0287  0.0379\n",
      "    104        1.0262        1.0284  0.0370\n",
      "    105        1.0258        1.0282  0.0360\n",
      "    106        1.0255        1.0279  0.0359\n",
      "    107        1.0252        1.0277  0.0369\n",
      "    108        1.0249        1.0274  0.0369\n",
      "    109        1.0245        1.0271  0.0376\n",
      "    110        1.0242        1.0269  0.0350\n",
      "    111        1.0239        1.0266  0.0389\n",
      "    112        1.0236        1.0263  0.0355\n",
      "    113        1.0233        1.0260  0.0369\n",
      "    114        1.0229        1.0257  0.0330\n",
      "    115        1.0226        1.0255  0.0329\n",
      "    116        1.0223        1.0252  0.0349\n",
      "    117        1.0220        1.0249  0.0360\n",
      "    118        1.0216        1.0246  0.0359\n",
      "    119        1.0213        1.0243  0.0369\n",
      "    120        1.0210        1.0240  0.0369\n",
      "    121        1.0207        1.0237  0.0349\n",
      "    122        1.0204        1.0234  0.0370\n",
      "    123        1.0200        1.0231  0.0350\n",
      "    124        1.0197        1.0228  0.0368\n",
      "    125        1.0194        1.0225  0.0350\n",
      "    126        1.0191        1.0222  0.0351\n",
      "    127        1.0187        1.0219  0.0350\n",
      "    128        1.0184        1.0216  0.0339\n",
      "    129        1.0181        1.0213  0.0351\n",
      "    130        1.0178        1.0210  0.0340\n",
      "    131        1.0174        1.0207  0.0339\n",
      "    132        1.0171        1.0204  0.0340\n",
      "    133        1.0168        1.0201  0.0349\n",
      "    134        1.0165        1.0199  0.0355\n",
      "    135        1.0161        1.0196  0.0340\n",
      "    136        1.0158        1.0193  0.0339\n",
      "    137        1.0155        1.0190  0.0339\n",
      "    138        1.0152        1.0187  0.0349\n",
      "    139        1.0149        1.0184  0.0339\n",
      "    140        1.0145        1.0181  0.0349\n",
      "    141        1.0142        1.0178  0.0340\n",
      "    142        1.0139        1.0175  0.0349\n",
      "    143        1.0136        1.0172  0.0350\n",
      "    144        1.0133        1.0169  0.0340\n",
      "    145        1.0129        1.0166  0.0379\n",
      "    146        1.0126        1.0163  0.0350\n",
      "    147        1.0123        1.0160  0.0379\n",
      "    148        1.0120        1.0157  0.0340\n",
      "    149        1.0117        1.0154  0.0389\n",
      "    150        1.0113        1.0151  0.0340\n",
      "    151        1.0110        1.0148  0.0350\n",
      "    152        1.0107        1.0145  0.0339\n",
      "    153        1.0104        1.0142  0.0349\n",
      "    154        1.0101        1.0139  0.0340\n",
      "    155        1.0098        1.0136  0.0340\n",
      "    156        1.0094        1.0133  0.0339\n",
      "    157        1.0091        1.0130  0.0340\n",
      "    158        1.0088        1.0127  0.0360\n",
      "    159        1.0085        1.0124  0.0338\n",
      "    160        1.0082        1.0121  0.0339\n",
      "    161        1.0079        1.0118  0.0350\n",
      "    162        1.0075        1.0115  0.0339\n",
      "    163        1.0072        1.0112  0.0339\n",
      "    164        1.0069        1.0110  0.0349\n",
      "    165        1.0066        1.0107  0.0340\n",
      "    166        1.0063        1.0104  0.0343\n",
      "    167        1.0060        1.0101  0.0350\n",
      "    168        1.0056        1.0098  0.0340\n",
      "    169        1.0053        1.0095  0.0342\n",
      "    170        1.0050        1.0092  0.0349\n",
      "    171        1.0047        1.0089  0.0341\n",
      "    172        1.0044        1.0086  0.0339\n",
      "    173        1.0041        1.0083  0.0359\n",
      "    174        1.0038        1.0080  0.0339\n",
      "    175        1.0034        1.0077  0.0350\n",
      "    176        1.0031        1.0074  0.0339\n",
      "    177        1.0028        1.0072  0.0340\n",
      "    178        1.0025        1.0069  0.0339\n",
      "    179        1.0022        1.0066  0.0339\n",
      "    180        1.0019        1.0063  0.0329\n",
      "    181        1.0016        1.0060  0.0329\n",
      "    182        1.0012        1.0057  0.0349\n",
      "    183        1.0009        1.0054  0.0339\n",
      "    184        1.0006        1.0052  0.0349\n",
      "    185        1.0003        1.0049  0.0340\n",
      "    186        1.0000        1.0046  0.0349\n",
      "    187        0.9997        1.0043  0.0359\n",
      "    188        0.9993        1.0040  0.0340\n",
      "    189        0.9990        1.0037  0.0349\n",
      "    190        0.9987        1.0034  0.0340\n",
      "    191        0.9984        1.0031  0.0340\n",
      "    192        0.9981        1.0029  0.0339\n",
      "    193        0.9978        1.0026  0.0349\n",
      "    194        0.9974        1.0023  0.0330\n",
      "    195        0.9971        1.0020  0.0340\n",
      "    196        0.9968        1.0017  0.0339\n",
      "    197        0.9965        1.0015  0.0350\n",
      "    198        0.9962        1.0012  0.0350\n",
      "    199        0.9959        1.0009  0.0349\n",
      "    200        0.9955        1.0006  0.0340\n",
      "    201        0.9952        1.0004  0.0340\n",
      "    202        0.9949        1.0001  0.0339\n",
      "    203        0.9946        0.9998  0.0334\n",
      "    204        0.9943        0.9995  0.0349\n",
      "    205        0.9940        0.9993  0.0339\n",
      "    206        0.9936        0.9990  0.0349\n",
      "    207        0.9933        0.9987  0.0339\n",
      "    208        0.9930        0.9985  0.0339\n",
      "    209        0.9927        0.9982  0.0340\n",
      "    210        0.9924        0.9979  0.0339\n",
      "    211        0.9921        0.9977  0.0339\n",
      "    212        0.9917        0.9974  0.0359\n",
      "    213        0.9914        0.9971  0.0339\n",
      "    214        0.9911        0.9969  0.0350\n",
      "    215        0.9908        0.9966  0.0329\n",
      "    216        0.9905        0.9964  0.0349\n",
      "    217        0.9902        0.9961  0.0329\n",
      "    218        0.9899        0.9958  0.0349\n",
      "    219        0.9896        0.9956  0.0339\n",
      "    220        0.9892        0.9953  0.0344\n",
      "    221        0.9889        0.9951  0.0355\n",
      "    222        0.9886        0.9948  0.0449\n",
      "    223        0.9883        0.9946  0.0399\n",
      "    224        0.9880        0.9943  0.0340\n",
      "    225        0.9877        0.9940  0.0379\n",
      "    226        0.9874        0.9938  0.0379\n",
      "    227        0.9871        0.9935  0.0379\n",
      "    228        0.9867        0.9933  0.0349\n",
      "    229        0.9864        0.9930  0.0369\n",
      "    230        0.9861        0.9927  0.0340\n",
      "    231        0.9858        0.9925  0.0340\n",
      "    232        0.9855        0.9922  0.0339\n",
      "    233        0.9852        0.9920  0.0340\n",
      "    234        0.9849        0.9917  0.0350\n",
      "    235        0.9846        0.9915  0.0337\n",
      "    236        0.9843        0.9913  0.0339\n",
      "    237        0.9840        0.9910  0.0335\n",
      "    238        0.9837        0.9908  0.0339\n",
      "    239        0.9834        0.9905  0.0340\n",
      "    240        0.9831        0.9903  0.0339\n",
      "    241        0.9828        0.9900  0.0339\n",
      "    242        0.9824        0.9898  0.0362\n",
      "    243        0.9821        0.9896  0.0340\n",
      "    244        0.9818        0.9893  0.0342\n",
      "    245        0.9815        0.9891  0.0349\n",
      "    246        0.9812        0.9889  0.0340\n",
      "    247        0.9809        0.9886  0.0350\n",
      "    248        0.9806        0.9884  0.0339\n",
      "    249        0.9803        0.9882  0.0340\n",
      "    250        0.9800        0.9880  0.0350\n",
      "    251        0.9797        0.9878  0.0339\n",
      "    252        0.9794        0.9875  0.0339\n",
      "    253        0.9791        0.9873  0.0349\n",
      "    254        0.9788        0.9871  0.0330\n",
      "    255        0.9785        0.9869  0.0329\n",
      "    256        0.9782        0.9867  0.0339\n",
      "    257        0.9779        0.9865  0.0360\n",
      "    258        0.9776        0.9863  0.0340\n",
      "    259        0.9773        0.9860  0.0339\n",
      "    260        0.9770        0.9858  0.0350\n",
      "    261        0.9767        0.9856  0.0340\n",
      "    262        0.9764        0.9854  0.0340\n",
      "    263        0.9761        0.9852  0.0349\n",
      "    264        0.9758        0.9850  0.0350\n",
      "    265        0.9755        0.9848  0.0339\n",
      "    266        0.9752        0.9846  0.0340\n",
      "    267        0.9749        0.9844  0.0339\n",
      "    268        0.9746        0.9842  0.0349\n",
      "    269        0.9743        0.9839  0.0339\n",
      "    270        0.9740        0.9837  0.0339\n",
      "    271        0.9737        0.9835  0.0340\n",
      "    272        0.9734        0.9833  0.0339\n",
      "    273        0.9731        0.9831  0.0339\n",
      "    274        0.9729        0.9829  0.0339\n",
      "    275        0.9726        0.9827  0.0340\n",
      "    276        0.9723        0.9825  0.0329\n",
      "    277        0.9720        0.9823  0.0342\n",
      "    278        0.9717        0.9821  0.0359\n",
      "    279        0.9714        0.9819  0.0339\n",
      "    280        0.9711        0.9817  0.0340\n",
      "    281        0.9708        0.9815  0.0340\n",
      "    282        0.9705        0.9813  0.0339\n",
      "    283        0.9702        0.9811  0.0339\n",
      "    284        0.9699        0.9809  0.0340\n",
      "    285        0.9696        0.9807  0.0349\n",
      "    286        0.9693        0.9805  0.0330\n",
      "    287        0.9690        0.9803  0.0349\n",
      "    288        0.9687        0.9801  0.0359\n",
      "    289        0.9684        0.9800  0.0340\n",
      "    290        0.9681        0.9798  0.0349\n",
      "    291        0.9678        0.9796  0.0339\n",
      "    292        0.9675        0.9794  0.0349\n",
      "    293        0.9672        0.9792  0.0340\n",
      "    294        0.9669        0.9790  0.0349\n",
      "    295        0.9666        0.9788  0.0339\n",
      "    296        0.9664        0.9786  0.0329\n",
      "    297        0.9661        0.9784  0.0349\n",
      "    298        0.9658        0.9782  0.0340\n",
      "    299        0.9655        0.9780  0.0339\n",
      "    300        0.9652        0.9778  0.0350\n",
      "    301        0.9649        0.9776  0.0339\n",
      "    302        0.9646        0.9775  0.0339\n",
      "    303        0.9643        0.9773  0.0330\n",
      "    304        0.9640        0.9771  0.0340\n",
      "    305        0.9637        0.9769  0.0339\n",
      "    306        0.9634        0.9767  0.0330\n",
      "    307        0.9631        0.9765  0.0330\n",
      "    308        0.9628        0.9763  0.0349\n",
      "    309        0.9625        0.9761  0.0349\n",
      "    310        0.9622        0.9760  0.0342\n",
      "    311        0.9619        0.9758  0.0359\n",
      "    312        0.9616        0.9756  0.0331\n",
      "    313        0.9613        0.9754  0.0349\n",
      "    314        0.9610        0.9752  0.0340\n",
      "    315        0.9607        0.9750  0.0339\n",
      "    316        0.9604        0.9748  0.0339\n",
      "    317        0.9601        0.9746  0.0332\n",
      "    318        0.9598        0.9744  0.0359\n",
      "    319        0.9596        0.9742  0.0350\n",
      "    320        0.9593        0.9740  0.0340\n",
      "    321        0.9590        0.9738  0.0340\n",
      "    322        0.9587        0.9736  0.0340\n",
      "    323        0.9584        0.9734  0.0339\n",
      "    324        0.9581        0.9732  0.0339\n",
      "    325        0.9578        0.9730  0.0339\n",
      "    326        0.9575        0.9728  0.0349\n",
      "    327        0.9572        0.9726  0.0340\n",
      "    328        0.9569        0.9724  0.0340\n",
      "    329        0.9566        0.9722  0.0330\n",
      "    330        0.9563        0.9720  0.0339\n",
      "    331        0.9560        0.9718  0.0349\n",
      "    332        0.9557        0.9716  0.0340\n",
      "    333        0.9554        0.9714  0.0349\n",
      "    334        0.9551        0.9712  0.0350\n",
      "    335        0.9548        0.9710  0.0340\n",
      "    336        0.9545        0.9708  0.0349\n",
      "    337        0.9542        0.9706  0.0330\n",
      "    338        0.9538        0.9704  0.0349\n",
      "    339        0.9535        0.9702  0.0339\n",
      "    340        0.9532        0.9700  0.0365\n",
      "    341        0.9529        0.9698  0.0329\n",
      "    342        0.9526        0.9696  0.0350\n",
      "    343        0.9523        0.9694  0.0349\n",
      "    344        0.9520        0.9692  0.0339\n",
      "    345        0.9517        0.9690  0.0339\n",
      "    346        0.9514        0.9688  0.0350\n",
      "    347        0.9511        0.9686  0.0339\n",
      "    348        0.9508        0.9684  0.0339\n",
      "    349        0.9505        0.9682  0.0341\n",
      "    350        0.9502        0.9680  0.0350\n",
      "    351        0.9499        0.9678  0.0332\n",
      "    352        0.9496        0.9676  0.0339\n",
      "    353        0.9493        0.9674  0.0340\n",
      "    354        0.9489        0.9672  0.0340\n",
      "    355        0.9486        0.9670  0.0350\n",
      "    356        0.9483        0.9668  0.0339\n",
      "    357        0.9480        0.9666  0.0349\n",
      "    358        0.9477        0.9664  0.0349\n",
      "    359        0.9474        0.9662  0.0349\n",
      "    360        0.9471        0.9660  0.0340\n",
      "    361        0.9468        0.9658  0.0339\n",
      "    362        0.9465        0.9656  0.0350\n",
      "    363        0.9461        0.9654  0.0340\n",
      "    364        0.9458        0.9652  0.0329\n",
      "    365        0.9455        0.9650  0.0339\n",
      "    366        0.9452        0.9648  0.0340\n",
      "    367        0.9449        0.9646  0.0339\n",
      "    368        0.9446        0.9644  0.0329\n",
      "    369        0.9443        0.9642  0.0350\n",
      "    370        0.9439        0.9640  0.0349\n",
      "    371        0.9436        0.9638  0.0340\n",
      "    372        0.9433        0.9636  0.0339\n",
      "    373        0.9430        0.9633  0.0350\n",
      "    374        0.9427        0.9631  0.0339\n",
      "    375        0.9424        0.9629  0.0349\n",
      "    376        0.9420        0.9627  0.0349\n",
      "    377        0.9417        0.9625  0.0340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    378        0.9414        0.9623  0.0350\n",
      "    379        0.9411        0.9620  0.0338\n",
      "    380        0.9408        0.9618  0.0330\n",
      "    381        0.9404        0.9616  0.0350\n",
      "    382        0.9401        0.9614  0.0349\n",
      "    383        0.9398        0.9612  0.0340\n",
      "    384        0.9395        0.9610  0.0349\n",
      "    385        0.9392        0.9608  0.0340\n",
      "    386        0.9388        0.9606  0.0350\n",
      "    387        0.9385        0.9604  0.0359\n",
      "    388        0.9382        0.9602  0.0349\n",
      "    389        0.9379        0.9600  0.0359\n",
      "    390        0.9375        0.9598  0.0349\n",
      "    391        0.9372        0.9596  0.0349\n",
      "    392        0.9369        0.9593  0.0389\n",
      "    393        0.9366        0.9591  0.0359\n",
      "    394        0.9362        0.9589  0.0339\n",
      "    395        0.9359        0.9587  0.0350\n",
      "    396        0.9356        0.9585  0.0339\n",
      "    397        0.9352        0.9583  0.0339\n",
      "    398        0.9349        0.9581  0.0339\n",
      "    399        0.9346        0.9578  0.0349\n",
      "    400        0.9343        0.9576  0.0349\n",
      "    401        0.9339        0.9574  0.0349\n",
      "    402        0.9336        0.9572  0.0349\n",
      "    403        0.9333        0.9569  0.0349\n",
      "    404        0.9329        0.9567  0.0340\n",
      "    405        0.9326        0.9565  0.0350\n",
      "    406        0.9323        0.9563  0.0360\n",
      "    407        0.9319        0.9561  0.0359\n",
      "    408        0.9316        0.9559  0.0350\n",
      "    409        0.9312        0.9556  0.0340\n",
      "    410        0.9309        0.9554  0.0359\n",
      "    411        0.9306        0.9552  0.0359\n",
      "    412        0.9302        0.9550  0.0349\n",
      "    413        0.9299        0.9548  0.0340\n",
      "    414        0.9295        0.9545  0.0349\n",
      "    415        0.9292        0.9543  0.0350\n",
      "    416        0.9289        0.9541  0.0340\n",
      "    417        0.9285        0.9539  0.0349\n",
      "    418        0.9282        0.9536  0.0340\n",
      "    419        0.9278        0.9534  0.0339\n",
      "    420        0.9275        0.9532  0.0349\n",
      "    421        0.9271        0.9530  0.0359\n",
      "    422        0.9268        0.9527  0.0349\n",
      "    423        0.9264        0.9525  0.0339\n",
      "    424        0.9261        0.9523  0.0345\n",
      "    425        0.9257        0.9520  0.0349\n",
      "    426        0.9254        0.9518  0.0334\n",
      "    427        0.9250        0.9516  0.0349\n",
      "    428        0.9247        0.9513  0.0342\n",
      "    429        0.9243        0.9511  0.0340\n",
      "    430        0.9240        0.9509  0.0339\n",
      "    431        0.9236        0.9506  0.0359\n",
      "    432        0.9233        0.9504  0.0340\n",
      "    433        0.9229        0.9502  0.0340\n",
      "    434        0.9226        0.9499  0.0339\n",
      "    435        0.9222        0.9497  0.0349\n",
      "    436        0.9219        0.9495  0.0359\n",
      "    437        0.9215        0.9492  0.0339\n",
      "    438        0.9211        0.9490  0.0349\n",
      "    439        0.9208        0.9487  0.0349\n",
      "    440        0.9204        0.9485  0.0339\n",
      "    441        0.9201        0.9483  0.0339\n",
      "    442        0.9197        0.9480  0.0339\n",
      "    443        0.9193        0.9478  0.0349\n",
      "    444        0.9190        0.9475  0.0339\n",
      "    445        0.9186        0.9473  0.0349\n",
      "    446        0.9182        0.9470  0.0359\n",
      "    447        0.9179        0.9468  0.0359\n",
      "    448        0.9175        0.9465  0.0339\n",
      "    449        0.9171        0.9463  0.0339\n",
      "    450        0.9168        0.9460  0.0350\n",
      "    451        0.9164        0.9458  0.0369\n",
      "    452        0.9160        0.9455  0.0339\n",
      "    453        0.9157        0.9453  0.0340\n",
      "    454        0.9153        0.9450  0.0339\n",
      "    455        0.9149        0.9448  0.0356\n",
      "    456        0.9146        0.9445  0.0349\n",
      "    457        0.9142        0.9443  0.0338\n",
      "    458        0.9138        0.9441  0.0340\n",
      "    459        0.9134        0.9438  0.0339\n",
      "    460        0.9131        0.9436  0.0339\n",
      "    461        0.9127        0.9434  0.0350\n",
      "    462        0.9123        0.9431  0.0339\n",
      "    463        0.9119        0.9429  0.0339\n",
      "    464        0.9115        0.9426  0.0339\n",
      "    465        0.9111        0.9424  0.0339\n",
      "    466        0.9108        0.9421  0.0349\n",
      "    467        0.9104        0.9419  0.0330\n",
      "    468        0.9100        0.9417  0.0349\n",
      "    469        0.9096        0.9414  0.0349\n",
      "    470        0.9092        0.9412  0.0349\n",
      "    471        0.9088        0.9409  0.0339\n",
      "    472        0.9084        0.9407  0.0350\n",
      "    473        0.9080        0.9404  0.0350\n",
      "    474        0.9077        0.9402  0.0350\n",
      "    475        0.9073        0.9399  0.0339\n",
      "    476        0.9069        0.9397  0.0340\n",
      "    477        0.9065        0.9394  0.0340\n",
      "    478        0.9061        0.9392  0.0339\n",
      "    479        0.9057        0.9389  0.0330\n",
      "    480        0.9053        0.9387  0.0349\n",
      "    481        0.9049        0.9384  0.0339\n",
      "    482        0.9045        0.9381  0.0340\n",
      "    483        0.9041        0.9378  0.0349\n",
      "    484        0.9037        0.9376  0.0339\n",
      "    485        0.9033        0.9373  0.0350\n",
      "    486        0.9029        0.9370  0.0349\n",
      "    487        0.9024        0.9368  0.0350\n",
      "    488        0.9020        0.9365  0.0349\n",
      "    489        0.9016        0.9362  0.0349\n",
      "    490        0.9012        0.9360  0.0349\n",
      "    491        0.9008        0.9357  0.0343\n",
      "    492        0.9004        0.9354  0.0339\n",
      "    493        0.9000        0.9352  0.0349\n",
      "    494        0.8996        0.9349  0.0340\n",
      "    495        0.8991        0.9346  0.0349\n",
      "    496        0.8987        0.9344  0.0342\n",
      "    497        0.8983        0.9341  0.0340\n",
      "    498        0.8979        0.9338  0.0342\n",
      "    499        0.8975        0.9335  0.0349\n",
      "    500        0.8970        0.9333  0.0345\n",
      "    501        0.8966        0.9330  0.0350\n",
      "    502        0.8962        0.9327  0.0349\n",
      "    503        0.8958        0.9325  0.0339\n",
      "    504        0.8953        0.9322  0.0339\n",
      "    505        0.8949        0.9319  0.0359\n",
      "    506        0.8945        0.9316  0.0349\n",
      "    507        0.8940        0.9314  0.0330\n",
      "    508        0.8936        0.9311  0.0339\n",
      "    509        0.8932        0.9308  0.0359\n",
      "    510        0.8927        0.9305  0.0339\n",
      "    511        0.8923        0.9302  0.0360\n",
      "    512        0.8919        0.9299  0.0339\n",
      "    513        0.8914        0.9296  0.0350\n",
      "    514        0.8910        0.9294  0.0349\n",
      "    515        0.8905        0.9291  0.0339\n",
      "    516        0.8901        0.9288  0.0349\n",
      "    517        0.8897        0.9285  0.0339\n",
      "    518        0.8892        0.9282  0.0329\n",
      "    519        0.8888        0.9280  0.0349\n",
      "    520        0.8883        0.9277  0.0349\n",
      "    521        0.8879        0.9274  0.0330\n",
      "    522        0.8874        0.9271  0.0359\n",
      "    523        0.8870        0.9268  0.0339\n",
      "    524        0.8865        0.9265  0.0330\n",
      "    525        0.8860        0.9262  0.0339\n",
      "    526        0.8856        0.9259  0.0349\n",
      "    527        0.8851        0.9256  0.0340\n",
      "    528        0.8847        0.9253  0.0351\n",
      "    529        0.8842        0.9250  0.0349\n",
      "    530        0.8837        0.9247  0.0339\n",
      "    531        0.8833        0.9244  0.0330\n",
      "    532        0.8828        0.9241  0.0339\n",
      "    533        0.8823        0.9239  0.0339\n",
      "    534        0.8819        0.9236  0.0349\n",
      "    535        0.8814        0.9233  0.0339\n",
      "    536        0.8809        0.9230  0.0339\n",
      "    537        0.8805        0.9227  0.0340\n",
      "    538        0.8800        0.9224  0.0339\n",
      "    539        0.8795        0.9220  0.0339\n",
      "    540        0.8790        0.9217  0.0349\n",
      "    541        0.8785        0.9214  0.0340\n",
      "    542        0.8781        0.9211  0.0350\n",
      "    543        0.8776        0.9208  0.0339\n",
      "    544        0.8771        0.9205  0.0349\n",
      "    545        0.8766        0.9202  0.0349\n",
      "    546        0.8761        0.9199  0.0329\n",
      "    547        0.8756        0.9196  0.0339\n",
      "    548        0.8751        0.9193  0.0339\n",
      "    549        0.8746        0.9190  0.0334\n",
      "    550        0.8742        0.9187  0.0349\n",
      "    551        0.8737        0.9184  0.0339\n",
      "    552        0.8732        0.9181  0.0339\n",
      "    553        0.8727        0.9178  0.0340\n",
      "    554        0.8722        0.9175  0.0349\n",
      "    555        0.8717        0.9172  0.0340\n",
      "    556        0.8712        0.9169  0.0350\n",
      "    557        0.8707        0.9166  0.0350\n",
      "    558        0.8702        0.9163  0.0331\n",
      "    559        0.8696        0.9160  0.0339\n",
      "    560        0.8691        0.9157  0.0344\n",
      "    561        0.8686        0.9153  0.0339\n",
      "    562        0.8681        0.9150  0.0369\n",
      "    563        0.8676        0.9147  0.0479\n",
      "    564        0.8671        0.9144  0.0449\n",
      "    565        0.8666        0.9141  0.0349\n",
      "    566        0.8661        0.9138  0.0339\n",
      "    567        0.8655        0.9135  0.0340\n",
      "    568        0.8650        0.9132  0.0339\n",
      "    569        0.8645        0.9128  0.0349\n",
      "    570        0.8640        0.9125  0.0349\n",
      "    571        0.8635        0.9122  0.0340\n",
      "    572        0.8629        0.9119  0.0340\n",
      "    573        0.8624        0.9116  0.0349\n",
      "    574        0.8619        0.9112  0.0350\n",
      "    575        0.8613        0.9109  0.0349\n",
      "    576        0.8608        0.9106  0.0339\n",
      "    577        0.8603        0.9103  0.0350\n",
      "    578        0.8597        0.9099  0.0339\n",
      "    579        0.8592        0.9096  0.0340\n",
      "    580        0.8587        0.9093  0.0340\n",
      "    581        0.8581        0.9090  0.0350\n",
      "    582        0.8576        0.9087  0.0339\n",
      "    583        0.8570        0.9083  0.0345\n",
      "    584        0.8565        0.9080  0.0339\n",
      "    585        0.8559        0.9077  0.0329\n",
      "    586        0.8554        0.9074  0.0340\n",
      "    587        0.8548        0.9071  0.0350\n",
      "    588        0.8543        0.9068  0.0339\n",
      "    589        0.8537        0.9064  0.0341\n",
      "    590        0.8532        0.9061  0.0339\n",
      "    591        0.8526        0.9058  0.0352\n",
      "    592        0.8521        0.9055  0.0340\n",
      "    593        0.8515        0.9052  0.0340\n",
      "    594        0.8509        0.9049  0.0339\n",
      "    595        0.8504        0.9046  0.0340\n",
      "    596        0.8498        0.9042  0.0350\n",
      "    597        0.8492        0.9039  0.0350\n",
      "    598        0.8487        0.9036  0.0349\n",
      "    599        0.8481        0.9033  0.0349\n",
      "    600        0.8475        0.9030  0.0339\n",
      "    601        0.8470        0.9027  0.0350\n",
      "    602        0.8464        0.9023  0.0349\n",
      "    603        0.8458        0.9020  0.0349\n",
      "    604        0.8452        0.9017  0.0359\n",
      "    605        0.8446        0.9014  0.0339\n",
      "    606        0.8440        0.9010  0.0349\n",
      "    607        0.8435        0.9007  0.0339\n",
      "    608        0.8429        0.9004  0.0329\n",
      "    609        0.8423        0.9000  0.0330\n",
      "    610        0.8417        0.8997  0.0340\n",
      "    611        0.8411        0.8994  0.0349\n",
      "    612        0.8405        0.8990  0.0359\n",
      "    613        0.8399        0.8987  0.0349\n",
      "    614        0.8393        0.8983  0.0345\n",
      "    615        0.8387        0.8980  0.0350\n",
      "    616        0.8381        0.8977  0.0340\n",
      "    617        0.8375        0.8973  0.0339\n",
      "    618        0.8369        0.8970  0.0339\n",
      "    619        0.8362        0.8966  0.0350\n",
      "    620        0.8356        0.8963  0.0340\n",
      "    621        0.8350        0.8960  0.0349\n",
      "    622        0.8344        0.8956  0.0350\n",
      "    623        0.8338        0.8953  0.0339\n",
      "    624        0.8332        0.8950  0.0340\n",
      "    625        0.8325        0.8946  0.0349\n",
      "    626        0.8319        0.8943  0.0339\n",
      "    627        0.8313        0.8940  0.0349\n",
      "    628        0.8307        0.8936  0.0350\n",
      "    629        0.8300        0.8933  0.0330\n",
      "    630        0.8294        0.8929  0.0349\n",
      "    631        0.8288        0.8926  0.0340\n",
      "    632        0.8281        0.8922  0.0340\n",
      "    633        0.8275        0.8919  0.0340\n",
      "    634        0.8268        0.8915  0.0349\n",
      "    635        0.8262        0.8912  0.0349\n",
      "    636        0.8255        0.8908  0.0349\n",
      "    637        0.8249        0.8905  0.0350\n",
      "    638        0.8242        0.8901  0.0349\n",
      "    639        0.8236        0.8897  0.0349\n",
      "    640        0.8229        0.8894  0.0349\n",
      "    641        0.8223        0.8890  0.0339\n",
      "    642        0.8216        0.8887  0.0339\n",
      "    643        0.8210        0.8883  0.0340\n",
      "    644        0.8203        0.8880  0.0330\n",
      "    645        0.8196        0.8876  0.0340\n",
      "    646        0.8190        0.8873  0.0339\n",
      "    647        0.8183        0.8870  0.0340\n",
      "    648        0.8176        0.8866  0.0349\n",
      "    649        0.8169        0.8863  0.0340\n",
      "    650        0.8163        0.8859  0.0340\n",
      "    651        0.8156        0.8856  0.0342\n",
      "    652        0.8149        0.8852  0.0349\n",
      "    653        0.8142        0.8849  0.0339\n",
      "    654        0.8135        0.8845  0.0339\n",
      "    655        0.8129        0.8842  0.0340\n",
      "    656        0.8122        0.8838  0.0350\n",
      "    657        0.8115        0.8835  0.0354\n",
      "    658        0.8108        0.8831  0.0349\n",
      "    659        0.8101        0.8827  0.0339\n",
      "    660        0.8094        0.8824  0.0350\n",
      "    661        0.8087        0.8820  0.0339\n",
      "    662        0.8080        0.8817  0.0330\n",
      "    663        0.8073        0.8813  0.0340\n",
      "    664        0.8066        0.8809  0.0339\n",
      "    665        0.8059        0.8805  0.0340\n",
      "    666        0.8052        0.8802  0.0340\n",
      "    667        0.8044        0.8798  0.0339\n",
      "    668        0.8037        0.8794  0.0339\n",
      "    669        0.8030        0.8791  0.0339\n",
      "    670        0.8023        0.8787  0.0339\n",
      "    671        0.8016        0.8784  0.0340\n",
      "    672        0.8008        0.8780  0.0339\n",
      "    673        0.8001        0.8777  0.0349\n",
      "    674        0.7994        0.8773  0.0349\n",
      "    675        0.7986        0.8770  0.0340\n",
      "    676        0.7979        0.8766  0.0340\n",
      "    677        0.7972        0.8763  0.0349\n",
      "    678        0.7964        0.8759  0.0339\n",
      "    679        0.7957        0.8756  0.0340\n",
      "    680        0.7950        0.8753  0.0339\n",
      "    681        0.7942        0.8750  0.0350\n",
      "    682        0.7935        0.8746  0.0340\n",
      "    683        0.7927        0.8743  0.0350\n",
      "    684        0.7920        0.8739  0.0340\n",
      "    685        0.7912        0.8736  0.0340\n",
      "    686        0.7905        0.8732  0.0350\n",
      "    687        0.7897        0.8729  0.0359\n",
      "    688        0.7890        0.8726  0.0357\n",
      "    689        0.7882        0.8722  0.0339\n",
      "    690        0.7874        0.8719  0.0353\n",
      "    691        0.7867        0.8715  0.0349\n",
      "    692        0.7859        0.8712  0.0349\n",
      "    693        0.7851        0.8709  0.0339\n",
      "    694        0.7844        0.8705  0.0350\n",
      "    695        0.7836        0.8702  0.0349\n",
      "    696        0.7828        0.8698  0.0349\n",
      "    697        0.7820        0.8695  0.0350\n",
      "    698        0.7812        0.8691  0.0339\n",
      "    699        0.7805        0.8688  0.0339\n",
      "    700        0.7797        0.8685  0.0359\n",
      "    701        0.7789        0.8681  0.0349\n",
      "    702        0.7781        0.8678  0.0340\n",
      "    703        0.7773        0.8674  0.0340\n",
      "    704        0.7765        0.8671  0.0339\n",
      "    705        0.7757        0.8668  0.0349\n",
      "    706        0.7749        0.8664  0.0349\n",
      "    707        0.7741        0.8661  0.0340\n",
      "    708        0.7733        0.8658  0.0339\n",
      "    709        0.7725        0.8655  0.0349\n",
      "    710        0.7717        0.8651  0.0340\n",
      "    711        0.7709        0.8648  0.0340\n",
      "    712        0.7701        0.8645  0.0340\n",
      "    713        0.7693        0.8641  0.0330\n",
      "    714        0.7685        0.8638  0.0330\n",
      "    715        0.7677        0.8635  0.0339\n",
      "    716        0.7669        0.8631  0.0349\n",
      "    717        0.7661        0.8628  0.0349\n",
      "    718        0.7652        0.8624  0.0349\n",
      "    719        0.7644        0.8621  0.0360\n",
      "    720        0.7636        0.8618  0.0349\n",
      "    721        0.7628        0.8614  0.0330\n",
      "    722        0.7620        0.8611  0.0339\n",
      "    723        0.7611        0.8607  0.0340\n",
      "    724        0.7603        0.8604  0.0339\n",
      "    725        0.7595        0.8600  0.0359\n",
      "    726        0.7587        0.8597  0.0350\n",
      "    727        0.7578        0.8593  0.0339\n",
      "    728        0.7570        0.8589  0.0339\n",
      "    729        0.7562        0.8586  0.0349\n",
      "    730        0.7553        0.8582  0.0330\n",
      "    731        0.7545        0.8579  0.0340\n",
      "    732        0.7536        0.8575  0.0340\n",
      "    733        0.7528        0.8572  0.0330\n",
      "    734        0.7520        0.8568  0.0349\n",
      "    735        0.7511        0.8564  0.0329\n",
      "    736        0.7503        0.8560  0.0359\n",
      "    737        0.7494        0.8557  0.0330\n",
      "    738        0.7486        0.8553  0.0352\n",
      "    739        0.7477        0.8550  0.0329\n",
      "    740        0.7469        0.8546  0.0345\n",
      "    741        0.7460        0.8543  0.0349\n",
      "    742        0.7452        0.8539  0.0340\n",
      "    743        0.7443        0.8536  0.0340\n",
      "    744        0.7434        0.8532  0.0340\n",
      "    745        0.7426        0.8529  0.0340\n",
      "    746        0.7417        0.8525  0.0343\n",
      "    747        0.7408        0.8521  0.0349\n",
      "    748        0.7400        0.8518  0.0340\n",
      "    749        0.7391        0.8514  0.0340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    750        0.7382        0.8511  0.0340\n",
      "    751        0.7374        0.8507  0.0340\n",
      "    752        0.7365        0.8504  0.0334\n",
      "    753        0.7356        0.8500  0.0340\n",
      "    754        0.7348        0.8497  0.0332\n",
      "    755        0.7339        0.8493  0.0340\n",
      "    756        0.7330        0.8490  0.0342\n",
      "    757        0.7321        0.8486  0.0361\n",
      "    758        0.7312        0.8483  0.0350\n",
      "    759        0.7304        0.8480  0.0353\n",
      "    760        0.7295        0.8476  0.0340\n",
      "    761        0.7286        0.8473  0.0354\n",
      "    762        0.7277        0.8469  0.0339\n",
      "    763        0.7268        0.8465  0.0339\n",
      "    764        0.7259        0.8462  0.0349\n",
      "    765        0.7250        0.8458  0.0349\n",
      "    766        0.7241        0.8455  0.0340\n",
      "    767        0.7232        0.8452  0.0340\n",
      "    768        0.7223        0.8448  0.0340\n",
      "    769        0.7214        0.8445  0.0340\n",
      "    770        0.7205        0.8441  0.0349\n",
      "    771        0.7196        0.8438  0.0349\n",
      "    772        0.7187        0.8435  0.0349\n",
      "    773        0.7178        0.8431  0.0339\n",
      "    774        0.7169        0.8428  0.0349\n",
      "    775        0.7160        0.8425  0.0339\n",
      "    776        0.7151        0.8421  0.0340\n",
      "    777        0.7142        0.8418  0.0340\n",
      "    778        0.7133        0.8415  0.0340\n",
      "    779        0.7124        0.8412  0.0340\n",
      "    780        0.7115        0.8408  0.0339\n",
      "    781        0.7106        0.8405  0.0360\n",
      "    782        0.7097        0.8402  0.0339\n",
      "    783        0.7087        0.8399  0.0329\n",
      "    784        0.7078        0.8395  0.0349\n",
      "    785        0.7069        0.8392  0.0339\n",
      "    786        0.7060        0.8389  0.0339\n",
      "    787        0.7051        0.8386  0.0351\n",
      "    788        0.7041        0.8383  0.0349\n",
      "    789        0.7032        0.8380  0.0349\n",
      "    790        0.7023        0.8376  0.0339\n",
      "    791        0.7014        0.8374  0.0340\n",
      "    792        0.7004        0.8370  0.0350\n",
      "    793        0.6995        0.8367  0.0340\n",
      "    794        0.6986        0.8363  0.0340\n",
      "    795        0.6976        0.8360  0.0340\n",
      "    796        0.6967        0.8357  0.0349\n",
      "    797        0.6958        0.8354  0.0340\n",
      "    798        0.6948        0.8351  0.0350\n",
      "    799        0.6939        0.8348  0.0359\n",
      "    800        0.6930        0.8344  0.0340\n",
      "    801        0.6920        0.8341  0.0349\n",
      "    802        0.6911        0.8338  0.0349\n",
      "    803        0.6901        0.8335  0.0359\n",
      "    804        0.6892        0.8332  0.0339\n",
      "    805        0.6882        0.8329  0.0340\n",
      "    806        0.6873        0.8325  0.0340\n",
      "    807        0.6863        0.8323  0.0339\n",
      "    808        0.6854        0.8320  0.0344\n",
      "    809        0.6844        0.8317  0.0340\n",
      "    810        0.6835        0.8314  0.0337\n",
      "    811        0.6825        0.8311  0.0329\n",
      "    812        0.6815        0.8308  0.0349\n",
      "    813        0.6806        0.8305  0.0359\n",
      "    814        0.6796        0.8301  0.0349\n",
      "    815        0.6787        0.8299  0.0349\n",
      "    816        0.6777        0.8295  0.0330\n",
      "    817        0.6767        0.8293  0.0340\n",
      "    818        0.6758        0.8290  0.0350\n",
      "    819        0.6748        0.8288  0.0339\n",
      "    820        0.6738        0.8284  0.0350\n",
      "    821        0.6729        0.8282  0.0339\n",
      "    822        0.6719        0.8278  0.0340\n",
      "    823        0.6709        0.8276  0.0340\n",
      "    824        0.6699        0.8272  0.0339\n",
      "    825        0.6690        0.8271  0.0339\n",
      "    826        0.6680        0.8266  0.0340\n",
      "    827        0.6670        0.8265  0.0349\n",
      "    828        0.6660        0.8261  0.0340\n",
      "    829        0.6650        0.8260  0.0349\n",
      "    830        0.6641        0.8255  0.0340\n",
      "    831        0.6631        0.8255  0.0340\n",
      "    832        0.6621        0.8249  0.0340\n",
      "    833        0.6611        0.8249  0.0340\n",
      "    834        0.6601        0.8244  0.0339\n",
      "    835        0.6592        0.8244  0.0340\n",
      "    836        0.6582        0.8239  0.0349\n",
      "    837        0.6572        0.8240  0.0339\n",
      "    838        0.6563        0.8236  0.0340\n",
      "    839        0.6553        0.8236  0.0340\n",
      "    840        0.6544        0.8233  0.0345\n",
      "    841        0.6535        0.8234  0.0349\n",
      "    842        0.6525        0.8234  0.0344\n",
      "    843        0.6517        0.8234  0.0340\n",
      "    844        0.6508        0.8238  0.0342\n",
      "    845        0.6502        0.8234  0.0340\n",
      "    846        0.6492        0.8249  0.0342\n",
      "    847        0.6490        0.8238  0.0350\n",
      "    848        0.6478        0.8267  0.0335\n",
      "    849        0.6480        0.8243  0.0339\n",
      "    850        0.6464        0.8284  0.0342\n",
      "    851        0.6471        0.8245  0.0349\n",
      "    852        0.6448        0.8294  0.0339\n",
      "    853        0.6458        0.8245  0.0370\n",
      "    854        0.6430        0.8293  0.0350\n",
      "    855        0.6440        0.8243  0.0350\n",
      "    856        0.6411        0.8291  0.0339\n",
      "    857        0.6422        0.8240  0.0339\n",
      "    858        0.6392        0.8290  0.0349\n",
      "    859        0.6403        0.8238  0.0349\n",
      "    860        0.6373        0.8291  0.0349\n",
      "    861        0.6386        0.8236  0.0340\n",
      "    862        0.6354        0.8290  0.0339\n",
      "    863        0.6367        0.8233  0.0349\n",
      "    864        0.6334        0.8289  0.0349\n",
      "    865        0.6348        0.8229  0.0349\n",
      "    866        0.6315        0.8291  0.0340\n",
      "    867        0.6330        0.8227  0.0339\n",
      "    868        0.6296        0.8295  0.0340\n",
      "    869        0.6313        0.8225  0.0349\n",
      "    870        0.6277        0.8296  0.0350\n",
      "    871        0.6295        0.8222  0.0339\n",
      "    872        0.6257        0.8296  0.0340\n",
      "    873        0.6276        0.8219  0.0339\n",
      "    874        0.6238        0.8300  0.0330\n",
      "    875        0.6258        0.8217  0.0350\n",
      "    876        0.6220        0.8306  0.0350\n",
      "    877        0.6241        0.8215  0.0340\n",
      "    878        0.6200        0.8310  0.0339\n",
      "    879        0.6224        0.8212  0.0340\n",
      "    880        0.6181        0.8310  0.0350\n",
      "    881        0.6205        0.8209  0.0340\n",
      "    882        0.6162        0.8318  0.0339\n",
      "    883        0.6188        0.8207  0.0349\n",
      "    884        0.6143        0.8324  0.0349\n",
      "    885        0.6171        0.8205  0.0350\n",
      "    886        0.6125        0.8329  0.0340\n",
      "    887        0.6154        0.8202  0.0339\n",
      "    888        0.6105        0.8331  0.0349\n",
      "    889        0.6135        0.8200  0.0340\n",
      "    890        0.6086        0.8337  0.0349\n",
      "    891        0.6117        0.8198  0.0369\n",
      "    892        0.6067        0.8344  0.0339\n",
      "    893        0.6100        0.8196  0.0349\n",
      "    894        0.6048        0.8351  0.0339\n",
      "    895        0.6083        0.8193  0.0369\n",
      "    896        0.6028        0.8352  0.0350\n",
      "    897        0.6064        0.8189  0.0339\n",
      "    898        0.6008        0.8354  0.0339\n",
      "    899        0.6045        0.8187  0.0349\n",
      "    900        0.5989        0.8360  0.0349\n",
      "    901        0.6027        0.8184  0.0350\n",
      "    902        0.5969        0.8362  0.0340\n",
      "    903        0.6008        0.8182  0.0339\n",
      "    904        0.5950        0.8369  0.0339\n",
      "    905        0.5991        0.8180  0.0352\n",
      "    906        0.5931        0.8377  0.0340\n",
      "    907        0.5973        0.8178  0.0339\n",
      "    908        0.5912        0.8387  0.0340\n",
      "    909        0.5956        0.8175  0.0340\n",
      "    910        0.5892        0.8388  0.0350\n",
      "    911        0.5936        0.8172  0.0349\n",
      "    912        0.5872        0.8388  0.0339\n",
      "    913        0.5915        0.8170  0.0340\n",
      "    914        0.5852        0.8394  0.0349\n",
      "    915        0.5896        0.8168  0.0339\n",
      "    916        0.5833        0.8404  0.0339\n",
      "    917        0.5878        0.8165  0.0340\n",
      "    918        0.5814        0.8415  0.0339\n",
      "    919        0.5860        0.8163  0.0339\n",
      "    920        0.5795        0.8422  0.0350\n",
      "    921        0.5842        0.8161  0.0349\n",
      "    922        0.5775        0.8432  0.0340\n",
      "    923        0.5823        0.8159  0.0339\n",
      "    924        0.5756        0.8443  0.0345\n",
      "    925        0.5806        0.8157  0.0359\n",
      "    926        0.5737        0.8450  0.0349\n",
      "    927        0.5787        0.8155  0.0340\n",
      "    928        0.5717        0.8455  0.0339\n",
      "    929        0.5767        0.8153  0.0340\n",
      "    930        0.5697        0.8461  0.0339\n",
      "    931        0.5748        0.8150  0.0350\n",
      "    932        0.5678        0.8471  0.0340\n",
      "    933        0.5730        0.8149  0.0341\n",
      "    934        0.5659        0.8486  0.0339\n",
      "    935        0.5713        0.8148  0.0342\n",
      "    936        0.5640        0.8498  0.0339\n",
      "    937        0.5696        0.8146  0.0349\n",
      "    938        0.5621        0.8503  0.0350\n",
      "    939        0.5677        0.8145  0.0340\n",
      "    940        0.5601        0.8511  0.0340\n",
      "    941        0.5658        0.8144  0.0350\n",
      "    942        0.5582        0.8524  0.0349\n",
      "    943        0.5641        0.8143  0.0350\n",
      "    944        0.5563        0.8535  0.0339\n",
      "    945        0.5624        0.8141  0.0350\n",
      "    946        0.5543        0.8538  0.0349\n",
      "    947        0.5604        0.8140  0.0340\n",
      "    948        0.5524        0.8547  0.0340\n",
      "    949        0.5586        0.8139  0.0340\n",
      "    950        0.5505        0.8554  0.0349\n",
      "    951        0.5567        0.8138  0.0340\n",
      "    952        0.5487        0.8562  0.0349\n",
      "    953        0.5549        0.8137  0.0380\n",
      "    954        0.5468        0.8574  0.0459\n",
      "    955        0.5531        0.8135  0.0439\n",
      "    956        0.5448        0.8570  0.0409\n",
      "    957        0.5510        0.8134  0.0399\n",
      "    958        0.5429        0.8585  0.0399\n",
      "    959        0.5493        0.8133  0.0399\n",
      "    960        0.5410        0.8597  0.0392\n",
      "    961        0.5476        0.8133  0.0409\n",
      "    962        0.5392        0.8614  0.0393\n",
      "    963        0.5459        0.8132  0.0389\n",
      "    964        0.5374        0.8628  0.0379\n",
      "    965        0.5443        0.8132  0.0352\n",
      "    966        0.5356        0.8649  0.0350\n",
      "    967        0.5427        0.8131  0.0359\n",
      "    968        0.5337        0.8652  0.0349\n",
      "    969        0.5408        0.8130  0.0350\n",
      "    970        0.5318        0.8658  0.0340\n",
      "    971        0.5389        0.8128  0.0339\n",
      "    972        0.5299        0.8661  0.0349\n",
      "    973        0.5369        0.8127  0.0340\n",
      "    974        0.5280        0.8664  0.0340\n",
      "    975        0.5350        0.8126  0.0340\n",
      "    976        0.5262        0.8675  0.0350\n",
      "    977        0.5332        0.8123  0.0349\n",
      "    978        0.5242        0.8674  0.0340\n",
      "    979        0.5311        0.8121  0.0339\n",
      "    980        0.5223        0.8685  0.0349\n",
      "    981        0.5292        0.8121  0.0350\n",
      "    982        0.5206        0.8710  0.0346\n",
      "    983        0.5277        0.8120  0.0340\n",
      "    984        0.5188        0.8720  0.0345\n",
      "    985        0.5260        0.8119  0.0340\n",
      "    986        0.5170        0.8735  0.0340\n",
      "    987        0.5243        0.8118  0.0350\n",
      "    988        0.5151        0.8746  0.0340\n",
      "    989        0.5225        0.8117  0.0339\n",
      "    990        0.5133        0.8761  0.0339\n",
      "    991        0.5209        0.8117  0.0352\n",
      "    992        0.5116        0.8780  0.0349\n",
      "    993        0.5193        0.8115  0.0342\n",
      "    994        0.5097        0.8783  0.0340\n",
      "    995        0.5174        0.8114  0.0342\n",
      "    996        0.5079        0.8786  0.0342\n",
      "    997        0.5155        0.8113  0.0340\n",
      "    998        0.5060        0.8792  0.0332\n",
      "    999        0.5136        0.8112  0.0359\n",
      "   1000        0.5042        0.8807  0.0342\n",
      "   1001        0.5120        0.8111  0.0339\n",
      "   1002        0.5025        0.8819  0.0352\n",
      "   1003        0.5102        0.8111  0.0339\n",
      "   1004        0.5008        0.8843  0.0350\n",
      "   1005        0.5087        0.8110  0.0359\n"
     ]
    }
   ],
   "source": [
    "model.set_params(max_epochs = 1000)\n",
    "\n",
    "_ = model.partial_fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cc3f139babc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# initialize_module(model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\fullenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PythonScripts\\reconproject\\cmriRecon\\models\\modelRepository.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBN32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#take BN out to avoid cycles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fullenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fullenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 338\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# initialize_module(model)\n",
    "model.module_(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 96, 96)\n",
      "(4, 20, 96, 96)\n",
      "(4, 20, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X)\n",
    "print(Y_pred[1].shape)\n",
    "print(Y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n",
      "0.5086947679519653\n",
      "0.8109692335128784\n"
     ]
    }
   ],
   "source": [
    "print(len(model.history))\n",
    "print(model.history[-1, 'train_loss'])\n",
    "print(model.history[-1, 'valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.003\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=alpha)\n",
    "\n",
    "\n",
    "#override previous\n",
    "for batch in trainloader:\n",
    "    input = batch['input']\n",
    "    target = batch['target']\n",
    "\n",
    "# move computation to gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# INCLUDE code for multiple GPUs !! #\n",
    "\n",
    "model = model.double().to(device)\n",
    "input = input.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "print(\"shape input\", input.shape)\n",
    "print(\"shape target\", target.shape)\n",
    "\n",
    "#tensorboardX setup\n",
    "curr_time = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "logdir = os.path.join(\"logs\", curr_time)\n",
    "logdir_train = os.path.join(logdir, \"train\")\n",
    "logdir_val = os.path.join(logdir, \"val\")\n",
    "train_summary_writer = SummaryWriter(logdir_train) # train SummaryWriter\n",
    "val_summary_writer = SummaryWriter(logdir_val)     # validation SummaryWriter\n",
    "\n",
    "print(\"\\n--------------------------------------------------\")\n",
    "print(\"current time marker: \", curr_time)\n",
    "print(\"Tensorboard log directory location:\", logdir)\n",
    "print(\"--------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  THE TRAINER FUNCTION NEEDS TO BE ADAPTED FROM THE myTrainer.py FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "#     for i, data in enumerate(sample, 0):\n",
    "        # get the inputs\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    output = model(input)\n",
    "    \n",
    "    loss = criterion(output, target)\n",
    "    train_summary_writer.add_scalar('train_loss', loss.item(), epoch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    print(epoch, \"\\t\", loss.item())\n",
    "#     running_loss += loss.item()\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "#             running_loss = 0.0\n",
    "#     print('[%d, %5d] loss: %.3f' %\n",
    "#           (epoch + 1, i + 1, running_loss / 2000))\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = best model (best val loss)\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files to matlab folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savetomatlab(target_dir, datatype, datatypestr, sample_number, overwrite = False): # e.g. savetomatlab(foo_dir, input)\n",
    "    \n",
    "    if overwrite == False:\n",
    "        print(\"Set overwrite = True to save the results !\")\n",
    "    \n",
    "#   print avoids to mistakenly overwrite previous results.\n",
    "    if overwrite == True:\n",
    "        output_path = ('C:/Users/littl/Documents/PythonScripts/reconproject/cmriRecon/results/' + target_dir)\n",
    "\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        spio.savemat(output_path + '/' + datatypestr + '_sample_' + str(sample_number), \\\n",
    "                {datatypestr + '_sample_' + str(sample_number) \\\n",
    "                 : datatype[sample_number,...]}) #dict w/ np value\n",
    "\n",
    "        print(\"Result images saved at:\\n\", output_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont forget to use:\n",
    "# input = myFunctions.imprepare(input);\n",
    "# output = myFunctions.imprepare(output);\n",
    "# target = myFunctions.imprepare(target);\n",
    "\n",
    "\n",
    "for i in range(len(input)):\n",
    "    savetomatlab('foo5', input, 'input', i)\n",
    "    savetomatlab('foo5', output, 'output', i)\n",
    "    savetomatlab('foo5', target, 'target', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input.shape)\n",
    "print(output.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training loss and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# documentation at https://www.tensorflow.org/tensorboard/r2/tensorboard_in_notebooks\n",
    "%tensorboard --logdir logs --port 6016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control TensorBoard display. If no port is provided, the most recently launched TensorBoard is used\n",
    "# notebook.display(port=6016)#, height=1000)\n",
    "# notebook.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for single sample loading\n",
    "\n",
    "sample = CMRIdataset[100]\n",
    "input = sample['input']\n",
    "target = sample['target']\n",
    "\n",
    "print(\"shape input\", input.shape)\n",
    "print(\"shape target\", target.shape)\n",
    "\n",
    "#to gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.from_numpy(input).float().unsqueeze(0).double().to(device)\n",
    "target = torch.from_numpy(target).float().unsqueeze(0).double().to(device)\n",
    "model = model.double().to(device)\n",
    "\n",
    "# reconstruct validation image\n",
    "output = model(input)\n",
    "print(sample_output.size)\n",
    "\n",
    "input = myFunctions.imprepare(input);\n",
    "output = myFunctions.imprepare(output);\n",
    "target = myFunctions.imprepare(target);\n",
    "\n",
    "print(sample_output.size)\n",
    "\n",
    "for i in range(len(input)):\n",
    "    savetomatlab('foo4val', input, 'input', i)\n",
    "    savetomatlab('foo4val', output, 'output', i)\n",
    "    savetomatlab('foo4val', target, 'target', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISPLAY IMAGES\n",
    "foo = []\n",
    "# reshape the images to fit them in the input\n",
    "# still need to figure out how to produce list of images.\n",
    "input_reshape = input.reshape([-1, 96, 96])\n",
    "for i in range(len(input)):\n",
    "    foo.append(input_reshape[i].cpu())\n",
    "    \n",
    "print(foo[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input.shape)\n",
    "\n",
    "# for i in range(len(input)):\n",
    "#     input[:,i,:,:]\n",
    "foo = []\n",
    "input_reshape = input.reshape([-1, 96, 96]).unsqueeze(1)\n",
    "print(input_reshape.shape)\n",
    "for i in range(len(input)):\n",
    "    foo.append(input_reshape[i].cpu())\n",
    "    \n",
    "\n",
    "grid_img = torchvision.utils.make_grid(input_reshape.cpu())\n",
    "\n",
    "print(input.reshape([-1,96,96]).shape)\n",
    "print(grid_img.shape)\n",
    "\n",
    "\n",
    "plt.imshow(grid_img)\n",
    "\n",
    "# plt.imshow(input.cpu().detach().numpy()[0,0,...])\n",
    "\n",
    "# plt.imshow(output.cpu().detach().numpy()[0,0,...])\n",
    "# plt.imshow(target.cpu().detach().numpy()[0,0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(npimg):\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(torch.from_numpy(input)))\n",
    "imshow(myFunctions.imprepare(torchvision.utils.make_grid(target)))\n",
    "imshow(myFunctions.imprepare(torchvision.utils.make_grid(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
